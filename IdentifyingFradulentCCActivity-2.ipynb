{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment is to identify fraudulent transactions from credit card data. The dataset is highly imbalanced, meaning the positive class (fraudulent transactions) only accounts for about 0.2% of the training data. This assignment will help you develop strategies for dealing with imbalanced datasets.\n",
    "Dataset description:\n",
    "The dataset contains credit card transactions from 2013. Due to privacy concerns, the actual features have been transformed using Principal Components Analysis (PCA). As such, nearly all of the features do not have intrinsic meaning. The only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction. The feature 'Class' is the target variable. “Class” takes a value of 1 in case of fraud and 0 otherwise. \n",
    "\n",
    "Training set: https://wikispaces.psu.edu/download/attachments/395383213/credit_card_train.csv?api=v2\n",
    "Test set: https://wikispaces.psu.edu/download/attachments/395383213/credit_card_test.csv?api=v2\n",
    " \n",
    "Evaluation metrics:\n",
    "For all experiments, use the following evaluation metric:\n",
    "Area under the receiver operating characteristic curve (AUC score) (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score)\n",
    "The final test set performance will also be evaluated using the AUC score. \n",
    "Instructions:\n",
    "Q1) Using 5-fold cross-validation, implement a very naive baseline classifier where the majority class (no fraud) is predicted for each sample. Report the mean and standard deviation of the AUC score in a table.\n",
    "Q2) Using 5-fold cross-validation, perform hyper parameter and model selection. Evaluate each of the following model:\n",
    "Random forest\n",
    "XGBOOST\n",
    "SVM\n",
    "KNN\n",
    "Naive Bayes\n",
    "Report the following:\n",
    "Provide the hyperparameter values tried, as well as the mean and standard deviation for the AUC score. Tune the following hyperparameters:\n",
    "Random forest: n_estimators\n",
    "XGBOOST: learning rate\n",
    "SVM: c (regularization penalty)\n",
    "KNN: number of neighbors\n",
    "Q3) Retrain the models from Q2 using cross-validation. This time train each model using the SMOTE algorithm. \n",
    "Tune the same parameters as the previous section.\n",
    "Report the mean and standard deviation for the AUC score for each model with SMOTE in a table.\n",
    "Q4) Identify the best performing model from all previous questions. Using 5-fold cross validation, plot the full ROC curve (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) against each validation fold. There should be five figures in total.\n",
    "Q5) Retrain on the best performing model from all previous questions on all the training data.  Predict on the test data. \n",
    "Describe the model selection process you used. \n",
    "Which model and why? \n",
    "Did you use oversampling?\n",
    "What hyperparameter values did you select?\n",
    "Describe the performance of your chosen model and parameter on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from six.moves import urllib \n",
    "import string \n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import * \n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "URL = 'https://wikispaces.psu.edu/download/attachments/395383213/credit_card_train.csv?api=v2'\n",
    "LOCAL_FOLDER = 'datasets'\n",
    "CC_TRAIN = 'credit_card_train.csv'\n",
    "CC_TRAIN_PATH = os.path.join(LOCAL_FOLDER, CC_TRAIN)\n",
    "os.makedirs(LOCAL_FOLDER, exist_ok = True)\n",
    "urllib.request.urlretrieve(URL, CC_TRAIN_PATH)\n",
    "URL = 'https://wikispaces.psu.edu/download/attachments/395383213/credit_card_test.csv?api=v2'\n",
    "LOCAL_FOLDER = 'datasets'\n",
    "CC_TEST = 'credit_card_test.csv'\n",
    "CC_TEST_PATH = os.path.join(LOCAL_FOLDER, CC_TEST)\n",
    "os.makedirs(LOCAL_FOLDER, exist_ok = True)\n",
    "urllib.request.urlretrieve(URL, CC_TEST_PATH)\n",
    "def load_cc_train(cc_train_path=CC_TRAIN_PATH):\n",
    "    csv_path = os.path.join(cc_train_path)\n",
    "    return pd.read_csv(csv_path)\n",
    "def load_cc_test(cc_test_path=CC_TEST_PATH):\n",
    "    csv_path = os.path.join(cc_test_path)\n",
    "    return pd.read_csv(csv_path)\n",
    "cc_train = load_cc_train()\n",
    "cc_test = load_cc_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cc_train['Class'] == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_train.iloc[:,:30]\n",
    "cc_train[['Class']]\n",
    "#need to normalize data using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182276 45569\n",
      "182276 45569\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = cc_train.iloc[:,:30]\n",
    "y = cc_train[['Class']]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y.values.ravel(), test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(len(X_train), len(X_test))\n",
    "print(len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(strategy='most_frequent')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = DummyClassifier(strategy='most_frequent')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.99829932 0.9983267  0.9983267  0.9983267  0.9983267 ]\n",
      "Mean: 0.998321227270225\n",
      "Standard Deviation: 1.0953771166910543e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9981566415765103"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print('Mean:', scores.mean())\n",
    "    print('Standard Deviation:', scores.std())\n",
    "    \n",
    "\n",
    "dummy_train_scores = cross_val_score(DummyClassifier(strategy = 'most_frequent'),\n",
    "                                                     X_train, y_train,\n",
    "                                                     scoring = 'accuracy', cv=5)\n",
    "display_scores(dummy_train_scores)\n",
    "prediction = model.predict(X_test)\n",
    "accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "y_pred_prob = model.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dummy_auc = cross_val_score(model, X_test, y_test, cv=5, scoring='roc_auc')\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print('Mean:', scores.mean())\n",
    "    print('Standard Deviation:', scores.std())\n",
    "display_scores(dummy_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost\n",
    "from sklearn import svm\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "svc = svm.SVC(probability=True)\n",
    "kn = KNeighborsClassifier()\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.fit(X_train, y_train)\n",
    "rf_model = rf.fit(X_train, y_train)\n",
    "svc_model = svc.fit(X_train, y_train)\n",
    "kn_model = kn.fit(X_train,y_train)\n",
    "gnb_model = gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC scores for validation set using 5 fold cross-validation\n",
    "cv_auc1 = cross_val_score(xgb_model, X_test, y_test, cv=5,scoring='roc_auc')\n",
    "cv_auc2 = cross_val_score(rf_model, X_test, y_test, cv=5,scoring='roc_auc')\n",
    "cv_auc3 = cross_val_score(svc_model, X_test, y_test, cv=5,scoring='roc_auc')\n",
    "cv_auc4 = cross_val_score(kn_model, X_test, y_test, cv=5,scoring='roc_auc')\n",
    "cv_auc5 = cross_val_score(gnb_model, X_test, y_test, cv=5,scoring='roc_auc')\n",
    "\n",
    "print('Cross-validation AUC score for XGB')\n",
    "display_scores(cv_auc1)\n",
    "print('\\n')\n",
    "print('Cross-validation AUC score for RF')\n",
    "display_scores(cv_auc2)\n",
    "print('\\n')\n",
    "print('Cross-validation AUC score for SVC')\n",
    "display_scores(cv_auc3)\n",
    "print('\\n')\n",
    "print('Cross-validation AUC score for KN')\n",
    "display_scores(cv_auc4)\n",
    "print('\\n')\n",
    "print('Cross-validation AUC score for GNB')\n",
    "display_scores(cv_auc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "gnb_predictions = gnb_model.predict(X_test)\n",
    "print(classification_report(y_test, gnb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "xgb_param_test = {'learning_rate': np.arange(start=0.02, stop = 1.2, step=0.02)}\n",
    "xgb_cv =GridSearchCV(xgb, param_grid=xgb_param_test,scoring='roc_auc',cv=5)\n",
    "xgb_cv.fit(X_train,y_train)\n",
    "for i in ['mean_test_score', 'std_test_score', 'params']:\n",
    "    print(i,\" : \",xgb_cv.cv_results_[i])\n",
    "print('Best parameter:',xgb_cv.best_params_ ,'/n',\"Best score:\", xgb_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_param_test = {'C': np.arange(0.06, 1.2, 0.03)}\n",
    "svc_cv=GridSearchCV(svc, param_grid=svc_param_test,scoring='roc_auc',cv=5)\n",
    "svc_cv.fit(X_train,y_train)\n",
    "for i in ['mean_test_score', 'std_test_score', 'params']:\n",
    "    print(i,\" : \",svc_cv.cv_results_[i])\n",
    "print('Best parameter:',svc_cv.best_params_ ,'/n',\"Best score:\", svc_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_param_test2 = {'C': np.arange(2, 27, 5)}\n",
    "svc_cv2 = GridSearchCV(svc, param_grid=svc_param_test2,scoring='roc_auc',cv=5)\n",
    "svc_cv2.fit(X_train,y_train)\n",
    "for i in ['mean_test_score', 'std_test_score', 'params']:\n",
    "    print(i,\" : \",svc_cv2.cv_results_[i])\n",
    "print('Best parameter:',svc_cv2.best_params_ ,'/n',\"Best score:\", svc_cv2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_test = {'n_estimators': np.arange(start=200, stop = 1200, step=200)}\n",
    "rf_cv=GridSearchCV(rf, param_grid=rf_param_test,scoring='roc_auc', cv=5)\n",
    "rf_cv.fit(X_train,y_train)\n",
    "for i in ['mean_test_score', 'std_test_score', 'params']:\n",
    "    print(i,\" : \",rf_cv.cv_results_[i])\n",
    "print('Best parameter:',rf_cv.best_params_ ,'/n',\"Best score:\", rf_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_param_test = {'n_neighbors': np.arange(start=2, stop = 200, step=10)}\n",
    "kn_cv=GridSearchCV(kn, param_grid=kn_param_test,scoring='roc_auc',cv=5, n_jobs=2)\n",
    "kn_cv.fit(X_train,y_train)\n",
    "for i in ['mean_test_score', 'std_test_score', 'params']:\n",
    "    print(i,\" : \",kn_cv.cv_results_[i])\n",
    "print('Best parameter:',kn_cv.best_params_ ,'/n',\"Best score:\", kn_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit models on training data based on parameters found from first round of parameter tuning\n",
    "xgb2 = XGBClassifier(learning_rate = 0.36000000000000004)\n",
    "xgb_model2 = xgb2.fit(X_train,y_train)\n",
    "rf2 = RandomForestClassifier(n_estimators=1000,n_jobs=-1)\n",
    "rf_model2 = rf2.fit(X_train,y_train)\n",
    "svc2 = svm.SVC(C=18, probability=True)\n",
    "svc_model2 = svc2.fit(X_train, y_train)\n",
    "kn2 = KNeighborsClassifier(n_neighbors=192, n_jobs=-1)\n",
    "kn_model2 = kn2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate auc scores on validation set using 5 fold cross-validation\n",
    "\n",
    "xgb_auc = cross_val_score(xgb_model2, X_test, y_test, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "rf_auc = cross_val_score(rf_model2, X_test, y_test, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "svc_auc = cross_val_score(svc_model2, X_test, y_test, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "kn_auc = cross_val_score(kn_model2, X_test, y_test, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "#print scores \n",
    "\n",
    "print('XGB AUC Scores from first round of parameter tuning')\n",
    "display_scores(xgb_auc)\n",
    "print('\\n')\n",
    "print('RF AUC Scores from first round of parameter tuning')\n",
    "display_scores(rf_auc)\n",
    "print('\\n')\n",
    "print('SVM AUC Scores from first round of parameter tuning')\n",
    "display_scores(svc_auc)\n",
    "print('\\n')\n",
    "print('KN AUC Scores from first round of parameter tuning')\n",
    "display_scores(kn_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilizing smote method \n",
    "from imblearn.over_sampling import SMOTE \n",
    "sm = SMOTE(random_state = 42) \n",
    "X_sm, y_sm = sm.fit_sample(X, y.values.ravel()) \n",
    "X_sm_train,y_sm_train = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning XGBClassifier hyperparameters trained on smote split set\n",
    "sm_xgb_param_test = {'learning_rate': np.arange(start=0.02, stop = 1.2, step=0.02)}\n",
    "sm_xgb_cv =GridSearchCV(xgb, param_grid=sm_xgb_param_test,scoring='roc_auc',cv=5)\n",
    "sm_xgb_cv.fit(X_sm_train,y_sm_train)\n",
    "for i in ['mean_test_score', 'std_test_score', 'params']:\n",
    "    print(i,\" : \",sm_xgb_cv.cv_results_[i])\n",
    "print('Best parameter:',sm_xgb_cv.best_params_ ,'/n',\"Best score:\", sm_xgb_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning Random Forest Classifier hyperparameters trained on smote\n",
    "sm_rf_param_test = {'n_estimators': np.arange(start=200, stop = 1200, step=200)}\n",
    "sm_rf_cv =GridSearchCV(rf,param_grid=sm_rf_param_test,scoring='roc_auc',cv=5, n_jobs=-1)\n",
    "sm_rf_cv.fit(X_sm_train,y_sm_train)\n",
    "\n",
    "for i in ['mean_test_score', 'std_test_score', 'params']:\n",
    "    print(i,\" : \",sm_rf_cv.cv_results_[i])\n",
    "print('Best parameter:',sm_rf_cv.best_params_ ,'/n',\"Best score:\", sm_rf_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning Support Vector Machine Classifier hyperparameters trained on smote split set\n",
    "sm_svc_param_test = {'C': np.arange(2, 22, 5)}\n",
    "sm_svc_cv = GridSearchCV(svc, param_grid=sm_svc_param_test,scoring='roc_auc',cv=5, n_jobs=4)\n",
    "sm_svc_cv.fit(X_sm_train,y_sm_train)\n",
    "for i in ['mean_test_score', 'std_test_score', 'params']:\n",
    "    print(i,\" : \",sm_svc_cv.cv_results_[i])\n",
    "print('Best parameter:',sm_svc_cv.best_params_ ,'\\n \"Best score:\"', sm_svc_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "sm_kn_param_test = {'n_neighbors': np.arange(start=10, stop = 200, step=10)}\n",
    "sm_kn_cv =GridSearchCV(kn, param_grid=sm_kn_param_test,scoring='roc_auc',cv=5, n_jobs=2)\n",
    "sm_kn_cv.fit(X_sm_train,y_sm_train)\n",
    "for i in ['mean_test_score', 'std_test_score', 'params']:\n",
    "    print(i,\" : \",sm_kn_cv.cv_results_[i])\n",
    "print('Best parameter:',sm_kn_cv.best_params_ ,'\\n \"Best score:\"', sm_kn_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "sm_kn_param_test = {'n_neighbors': np.arange(start=1, stop = 10, step=1)}\n",
    "sm_kn_cv =GridSearchCV(kn, param_grid=sm_kn_param_test,scoring='roc_auc',cv=5, n_jobs=2)\n",
    "sm_kn_cv.fit(X_sm_train,y_sm_train)\n",
    "for i in ['mean_test_score', 'std_test_score', 'params']:\n",
    "    print(i,\" : \",sm_kn_cv.cv_results_[i])\n",
    "print('Best parameter:',sm_kn_cv.best_params_ ,'\\n \"Best score:\"', sm_kn_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit models based on parameters found in second round of parameter tuning \n",
    "\n",
    "xgb3 = XGBClassifier(learning_rate=0.54, n_jobs=-1)\n",
    "rf3 = RandomForestClassifier(n_estimators=400, n_jobs=-1)\n",
    "kn3 = KNeighborsClassifier(n_neighbors=5,n_jobs=-1 )\n",
    "#train models on smote data\n",
    "xgb_model3 = xgb3.fit(X_sm_train, y_sm_train)\n",
    "rf_model3=rf3.fit(X_sm_train, y_sm_train)\n",
    "#svm_model3=svm.SVC(C=)\n",
    "kn_model3=kn3.fit(X_sm_train, y_sm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate auc scores on validation set using 5 fold cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "xgb_auc2 = cross_val_score(xgb_model3, X_test, y_test, cv=5, scoring='roc_auc', n_jobs=4)\n",
    "rf_auc2 = cross_val_score(rf_model3, X_test, y_test, cv=5, scoring='roc_auc', n_jobs=4)\n",
    "#svm_auc2 = cross_val_score(svm_clf, X_test, y_test, cv=5, scoring='roc_auc')\n",
    "kn_auc2 = cross_val_score(kn_model3, X_test, y_test, cv=5, scoring='roc_auc')\n",
    "\n",
    "#print scores \n",
    "\n",
    "print('XGB AUC Scores from second round of parameter tuning')\n",
    "display_scores(xgb_auc2)\n",
    "print('\\n')\n",
    "print('RF AUC Scores from second round of parameter tuning')\n",
    "display_scores(rf_auc2)\n",
    "print('\\n')\n",
    "#print('SVM AUC Scores from first round of parameter tuning')\n",
    "#display_scores(svm_auc)\n",
    "#print('\\n')\n",
    "print('KN AUC Scores from second round of parameter tuning')\n",
    "display_scores(kn_auc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "prediction1 = xgb_model3.predict(X_test)\n",
    "prediction2 = rf_model3.predict(X_test)\n",
    "prediction3 = rf_model2.predict(X_test)\n",
    "print('Accuracy for XGB model trained on smote:',accuracy_score(prediction1, y_test))\n",
    "print('Accuracy for RF model trained on smote:',accuracy_score(prediction2, y_test))\n",
    "print('Accuracy for RF model not trained on smote:',accuracy_score(prediction3, y_test))\n",
    "print('Classification report for XGB model trained on smote')\n",
    "classification_report(y_test, prediction1)\n",
    "print('\\n')\n",
    "print('Classification report for RF model trained on smote') \n",
    "(classification_report(y_test, prediction2))\n",
    "print('\\n')\n",
    "print('Classification report for RF model not trained on smote')\n",
    "classification_report(y_test, prediction3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "cv = KFold(n_splits=5)\n",
    "\n",
    "for train,test in cv.split(X,y):\n",
    "    prob = RandomForestClassifier(n_estimators=1000, n_jobs=-1).fit(X.loc[train],y.loc[train].values.ravel()).predict_proba(X.loc[test])[:,1]\n",
    "    fpr, tpr, t = roc_curve(y.loc[test], prob)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "    i= i+1\n",
    "plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "         label=r'Mean ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_best = RandomForestClassifier(n_estimators=1000, n_jobs=-1)\n",
    "second_best = XGBClassifier(learning_rate=0.54) #trained on smote\n",
    "first_train = first_best.fit(X_train, y_train)\n",
    "second_train = second_best.fit(X_sm_train, y_sm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.9995409  0.90388234 0.9948076  0.96670848 0.99987633]\n",
      "Mean: 0.9729631294091782\n",
      "Standard Deviation: 0.03665830059909268\n",
      "Scores: [0.99857096 0.93419938 0.97705772 0.95749083 0.99973205]\n",
      "Mean: 0.9734101901725843\n",
      "Standard Deviation: 0.025020482606171044\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "first_cv = cross_val_score(first_best, X_test, y_test, cv=5, scoring='roc_auc')\n",
    "second_cv = cross_val_score(second_best, X_test, y_test, cv=5, scoring='roc_auc')\n",
    "display_scores(first_cv)\n",
    "display_scores(second_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     45485\n",
      "           1       0.94      0.81      0.87        84\n",
      "\n",
      "    accuracy                           1.00     45569\n",
      "   macro avg       0.97      0.90      0.94     45569\n",
      "weighted avg       1.00      1.00      1.00     45569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     45485\n",
      "           1       0.90      0.85      0.87        84\n",
      "\n",
      "    accuracy                           1.00     45569\n",
      "   macro avg       0.95      0.92      0.94     45569\n",
      "weighted avg       1.00      1.00      1.00     45569\n",
      "\n",
      "0.9995611051372644\n",
      "0.9995391603941276\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred=first_train.predict(X_test)\n",
    "pred2 = second_train.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "print(classification_report(y_test, pred2))\n",
    "print(accuracy_score(y_test, pred))\n",
    "print(accuracy_score(y_test, pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "         0\n",
      "0      0.0\n",
      "1      0.0\n",
      "2      0.0\n",
      "3      0.0\n",
      "4      0.0\n",
      "...    ...\n",
      "45564  0.0\n",
      "45565  0.0\n",
      "45566  0.0\n",
      "45567  0.0\n",
      "45568  0.0\n",
      "\n",
      "[45569 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred_prob = first_train.predict_proba(X_test)[:,1]\n",
    "print(roc_auc_score(y_test, y_pred_prob))\n",
    "print(pd.DataFrame(y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = first_best.fit(X, y.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = best_model.predict_proba(cc_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "richard_cruz_labels = pd.DataFrame(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56957</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56958</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56959</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56960</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56961</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56962 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "0      0.0\n",
       "1      0.0\n",
       "2      0.0\n",
       "3      0.0\n",
       "4      0.0\n",
       "...    ...\n",
       "56957  0.0\n",
       "56958  0.0\n",
       "56959  0.0\n",
       "56960  0.0\n",
       "56961  0.0\n",
       "\n",
       "[56962 rows x 1 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "richard_cruz_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "richard_cruz_labels.to_csv(r'richard_cruz_labels.csv',header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final AUC Score on unlabled test data: 0.96062"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
